{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Card_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RITGWCd0M9oL",
        "outputId": "b35e698d-0092-4cbd-d228-76aa46962f83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os,sys\n",
        "sys.path.append('/content/drive/MyDrive/iapr/project/')\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "from typing import Union\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import torchvision.models as models\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE=32\n",
        "print('CUDA INDEX: {}'.format(DEVICE.index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyozAeXYNoVk",
        "outputId": "93989fc6-efd7-4157-c863-e60b895b4440"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA INDEX: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7066OovCNtzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "fMKei9_nOXvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class Iapr(Dataset):\n",
        "#     def __init__(self, ims_obj, labels, transform=None):\n",
        "#         self.transform = transform\n",
        "#         self.labels=labels\n",
        "#         self.ims_obj=np.array(ims_obj)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         img = self.ims_obj[index]\n",
        "#         if self.transform is not None:\n",
        "#           img = self.transform(img)\n",
        "#         label = self.labels[index]\n",
        "\n",
        "#         return img,label\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.ims_obj.shape[0]\n",
        "\n",
        "class Iapr(Dataset):\n",
        "    def __init__(self, ims_obj, labels, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = img_obj[index] #to convert grayscale images to RGB.\n",
        "\n",
        "        if self.transform is not None:\n",
        "          img = self.transform(img)\n",
        "        \n",
        "        label = labels[index]\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return labels.shape[0]"
      ],
      "metadata": {
        "id": "JaHGmn8COZTw"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformers\n",
        "custom_transform = transforms.Compose([#transforms.Lambda(lambda x: x/255.),# not necessary\n",
        "                                       transforms.Resize((224, 224)),\n",
        "                                       #transforms.RandomCrop((224, 224)),\n",
        "                                       #transforms.ColorJitter(brightness=0.5),\n",
        "                                       #transforms.RandomRotation(degrees=45),\n",
        "                                       #transforms.RandomHorizontalFlip(p=0.1),\n",
        "                                       #transforms.RandomVerticalFlip(p=0.5),\n",
        "                                       #transforms.RandomGrayscale(p=0.05),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))\n",
        "                                       \n",
        "                                      ])\n",
        "\n",
        "test_transform = transforms.Compose([#transforms.Lambda(lambda x: x/255.),# not necessary\n",
        "                                       transforms.Resize((224, 224)),\n",
        "                                       #transforms.RandomCrop((224, 224)),\n",
        "                                       #transforms.ColorJitter(brightness=0.5),\n",
        "                                       #transforms.RandomRotation(degrees=45),\n",
        "                                       #transforms.RandomHorizontalFlip(p=0.1),\n",
        "                                       #transforms.RandomVerticalFlip(p=0.5),\n",
        "                                       #transforms.RandomGrayscale(p=0.05),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))\n",
        "                                      ])"
      ],
      "metadata": {
        "id": "lIwK2oEXPMaz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "6O5f5r0FObP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.features = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(3, 256, kernel_size=11, stride=4, padding=2),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            #\n",
        "            torch.nn.Conv2d(256, 192, kernel_size=5, padding=2),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            #\n",
        "            torch.nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            #\n",
        "            torch.nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            #\n",
        "            torch.nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = torch.nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Dropout(0.5),\n",
        "            torch.nn.Linear(256 * 6 * 6, 4096),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Dropout(0.5),\n",
        "            torch.nn.Linear(4096, 4096),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), 256 * 6 * 6)\n",
        "        logits = self.classifier(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "-chzH6njP-fv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "bO7yKeW4O5gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(model, data_loader):\n",
        "  correct_pred, num_examples = 0, 0\n",
        "  for i, (features, targets) in enumerate(data_loader):            \n",
        "      features = features.to(DEVICE)\n",
        "      targets = targets.to(DEVICE)\n",
        "      logits = model(features)\n",
        "      _, predicted_labels = torch.max(logits, 1)\n",
        "      num_examples += targets.size(0)\n",
        "      correct_pred += (predicted_labels == targets).sum()\n",
        "  return correct_pred.float()/num_examples * 100\n"
      ],
      "metadata": {
        "id": "lbL6OeksO_HX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, num_epochs, train_loader,\n",
        "                valid_loader, test_loader, optimizer, device):\n",
        "  start_time = time.time()\n",
        "  minibatch_loss_list, train_acc_list, valid_acc_list = [], [], []\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "      model.train()\n",
        "      for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "\n",
        "          features = features.to(device)\n",
        "          targets = targets.to(device, dtype=torch.int64)\n",
        "\n",
        "          # ## FORWARD AND BACK PROP\n",
        "          logits = model(features)\n",
        "          loss = torch.nn.functional.cross_entropy(logits, targets)\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          loss.backward()\n",
        "\n",
        "          # ## UPDATE MODEL PARAMETERS\n",
        "          optimizer.step()\n",
        "\n",
        "          # ## LOGGING\n",
        "          minibatch_loss_list.append(loss.item())\n",
        "          # if batch_idx % 50:\n",
        "          #     print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
        "          #           f'| Batch {batch_idx+1:04d}/{len(train_loader):04d} '\n",
        "          #           f'| Loss: {loss:.4f}')\n",
        "#################################################################################################################################\n",
        "      model.eval()\n",
        "      with torch.no_grad():  # save memory during inference\n",
        "          train_acc = compute_accuracy(model, train_loader#, device=device\n",
        "                                        )\n",
        "          valid_acc = compute_accuracy(model, valid_loader#, device=device\n",
        "                                        )\n",
        "          print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
        "                f'| Train: {train_acc :.2f}% '\n",
        "                f'| Validation: {valid_acc :.2f}%')\n",
        "          train_acc_list.append(train_acc.item())\n",
        "          valid_acc_list.append(valid_acc.item())\n",
        "\n",
        "      elapsed = (time.time() - start_time)/60\n",
        "      print(f'Time elapsed: {elapsed:.2f} min')\n",
        "\n",
        "  elapsed = (time.time() - start_time)/60\n",
        "  print(f'Total Training Time: {elapsed:.2f} min')\n",
        "\n",
        "  #test_acc = compute_accuracy(model, test_loader#, device=device)\n",
        "  #print(f'Test accuracy {test_acc :.2f}%')\n",
        "\n",
        "  return minibatch_loss_list, train_acc_list, valid_acc_list"
      ],
      "metadata": {
        "id": "8mUOg8ZfOc8g"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline"
      ],
      "metadata": {
        "id": "p5vyYvVyPIDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dictonary from labels(str) to int\n",
        "full_dict_iapr={\n",
        "  'AS': 0, '2S':1, '3S':2, '4S':3, '5S':4, '6S':5, '7S':6,\n",
        "  '8S':7, '9S':8, '10S':9, 'JS':10, 'QS':11, 'KS':12,\n",
        "  'AH': 13, '2H': 14, '3H': 15, '4H': 16, '5H': 17, '6H': 18,\n",
        "  '7H':19, '8H':20, '9H':21, '10H':22, 'JH': 23, 'QH': 24,\n",
        "  'KH':25,\n",
        "  'AC': 26, '2C': 27, '3C': 28, '4C': 29, '5C': 30, '6C': 31,\n",
        "  '7C':32, '8C':33, '9C':34, '10C':35, 'JC': 36, 'QC': 37,\n",
        "  'KC':38,\n",
        "  'AD': 39, '2D': 40, '3D': 41, '4D': 42, '5D': 43, '6D': 44,\n",
        "  '7D':45, '8D':46, '9D':47, '10D':48, 'JD': 49, 'QD': 50,\n",
        "  'KD':51}"
      ],
      "metadata": {
        "id": "YHnwd_hMO77m"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Raw data link**\n",
        "\n",
        "  [Label: updated_train_labels.csv](https://drive.google.com/file/d/1L8TglWAo9wKQNXJvVbYkbQIa3wJOLEEO/view?usp=sharing)\n",
        "\n",
        "  [Data: Tcards.npy](https://drive.google.com/file/d/1eVEAf7qZaTpfqygPjPsCIT5nNrsKcMk7/view?usp=sharing)"
      ],
      "metadata": {
        "id": "mhSKydUoTXwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load raw data\n",
        "import pandas as pd\n",
        "path = '/content/drive/MyDrive/iapr/project/data/train'\n",
        "df = pd.read_csv(os.path.join(path,'updated_train_labels.csv')) #filelink: https://drive.google.com/file/d/1eVEAf7qZaTpfqygPjPsCIT5nNrsKcMk7/view?usp=sharing\n",
        "\n",
        "labels=[]\n",
        "for i in range(28):\n",
        "  for t in ['T1','T2','T3','T4','T5']:\n",
        "    tmp = df.iloc[i]\n",
        "    labels.append(full_dict_iapr[tmp[t]])\n",
        "labels = np.array(labels)\n",
        "\n",
        "import PIL.Image as Image\n",
        "a=transforms.Resize([224, 224])\n",
        "arr=np.load(\"/content/drive/MyDrive/iapr/project/data/Tcards.npy\") # 140,348,228,3 Num,H,W,C\n",
        "img_obj = [Image.fromarray(im.astype(np.uint8)) for im in arr]\n",
        "print('img_obj len:{}'.format(len(img_obj)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8FOkrM3PsG6",
        "outputId": "6d3b8888-702c-4d5f-ef71-0afe4f198ef0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img_obj len:140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construct dataset\n",
        "train_dataset = Iapr(img_obj[0:120],labels,    transform=custom_transform)\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          drop_last=True,\n",
        "                          shuffle=True, # want to shuffle the dataset\n",
        "                          num_workers=2) # number processes/CPUs to use\n",
        "valid_dataset = Iapr(\n",
        "    img_obj[120:130],\n",
        "    labels,\n",
        "    transform=test_transform)\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    dataset=valid_dataset,\n",
        "    batch_size=12,\n",
        "    shuffle=False,\n",
        "    num_workers=2)\n",
        "\n",
        "test_dataset = Iapr(\n",
        "    img_obj[-10],\n",
        "    labels,\n",
        "    transform=test_transform)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2)\n"
      ],
      "metadata": {
        "id": "vbVhWACAPsjj"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pretrained model from Kaggele**\n",
        "\n",
        "[Model.pt](https://drive.google.com/file/d/1Ehoyi5n9oYqw1oLB_yVEDASeZMW9Z66F/view?usp=sharing)"
      ],
      "metadata": {
        "id": "kfVErAjfUFzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model\n",
        "path='/content/drive/MyDrive/iapr/project/data_kaggle'\n",
        "model=AlexNet(num_classes=52)\n",
        "model = model.to(DEVICE)\n",
        "model.load_state_dict(torch.load(path+'/model.pt'))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8y9qgghP60Q",
        "outputId": "564bdbb7-8c7d-4bda-d162-79b6126d1315"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 256, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(256, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=52, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tune"
      ],
      "metadata": {
        "id": "3TQESfD4Sc8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train and save\n",
        "optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.1)\n",
        "\n",
        "minibatch_loss_list, train_acc_list, valid_acc_list = train_model(\n",
        "    model=model,\n",
        "    num_epochs=50,\n",
        "    train_loader=train_loader,\n",
        "    valid_loader=valid_loader,\n",
        "    test_loader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    device=DEVICE, \n",
        "    )\n",
        "\n",
        "time=time.strftime('%Y-%m-%d-%H-%M', time.localtime())\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/iapr/project/'+'/model/'+time+'.pt')"
      ],
      "metadata": {
        "id": "yPlOUbRFP7NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use fine-tuned model"
      ],
      "metadata": {
        "id": "_9oFw8l8SQwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-tuned Model**\n",
        "\n",
        "[Model.pt](https://drive.google.com/file/d/14DJ4M4eEOAncIsyhzzTmh6MqTyFFp5PE/view?usp=sharing)"
      ],
      "metadata": {
        "id": "1HPYjp4DUXDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=AlexNet(num_classes=52)\n",
        "model = model.to(DEVICE)\n",
        "model.load_state_dict(torch.load(path+'/model.pt'))\n",
        "model.eval()\n",
        "test_acc = compute_accuracy(model, test_loader)"
      ],
      "metadata": {
        "id": "oOICu29JQaee"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDONP0pDTFCG",
        "outputId": "40987365-3823-446d-e03c-5b58215dcd11"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(82.8571, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ]
}